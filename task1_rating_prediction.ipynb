{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "059b9f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de8b4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb4c6653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We got here around midnight last Friday... the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brought a friend from Louisiana here.  She say...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Every friday, my dad and I eat here. We order ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My husband and I were really, really disappoin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Love this place!  Was in phoenix 3 weeks for w...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  We got here around midnight last Friday... the...      4\n",
       "1  Brought a friend from Louisiana here.  She say...      5\n",
       "2  Every friday, my dad and I eat here. We order ...      3\n",
       "3  My husband and I were really, really disappoin...      1\n",
       "4  Love this place!  Was in phoenix 3 weeks for w...      5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"yelp.csv\")\n",
    "\n",
    "df = df[['text', 'stars']]\n",
    "df = df.sample(200, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2cda246d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_llm\u001b[39m(prompt):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m         response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      4\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/llama-3.1-8b-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m             messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m             max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[0;32m     11\u001b[0m         )\n",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_llm\u001b[39m(prompt):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m         response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      4\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/llama-3.1-8b-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m             messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m             max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[0;32m     11\u001b[0m         )\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython.pyx:1112\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython.pyx:1090\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2188\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2185\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2187\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2188\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2190\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2193\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2257\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2254\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[0;32m   2255\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[1;32m-> 2257\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2258\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m   2260\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:600\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    598\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 600\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def call_llm(prompt):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/llama-3.1-8b-instruct\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a strict JSON API.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=200\n",
    "        )\n",
    "\n",
    "        text = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Remove markdown if present\n",
    "        text = re.sub(r\"```json|```\", \"\", text).strip()\n",
    "\n",
    "        # Extract JSON safely\n",
    "        start = text.find(\"{\")\n",
    "        end = text.rfind(\"}\") + 1\n",
    "\n",
    "        if start == -1 or end == -1:\n",
    "            return None\n",
    "\n",
    "        return text[start:end]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"LLM Error:\", e)\n",
    "        return None\n",
    "l;pp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43ba848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_v1 = \"\"\"\n",
    "Read the Yelp review below and predict a star rating from 1 to 5.\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "  \"predicted_stars\": 1-5,\n",
    "  \"explanation\": \"short reason\"\n",
    "}}\n",
    "\n",
    "Review:\n",
    "\"{review}\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3819fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_v2 = \"\"\"\n",
    "You are a sentiment analysis expert.\n",
    "\n",
    "Analyze sentiment, service, and quality.\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "  \"predicted_stars\": 1-5,\n",
    "  \"explanation\": \"short reason\"\n",
    "}}\n",
    "\n",
    "Review:\n",
    "\"{review}\"\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc693880",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_v3 = \"\"\"\n",
    "You are a strict JSON generator.\n",
    "\n",
    "Rules:\n",
    "- Output ONLY valid JSON\n",
    "- No extra text\n",
    "\n",
    "JSON format:\n",
    "{{\n",
    "  \"predicted_stars\": integer (1-5),\n",
    "  \"explanation\": string (max 20 words)\n",
    "}}\n",
    "\n",
    "Review:\n",
    "\"{review}\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b4fa442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predict_fn, df):\n",
    "    correct = 0\n",
    "    valid_json = 0\n",
    "    total = len(df)\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=total):\n",
    "        raw = predict_fn(row['text'])\n",
    "\n",
    "        if raw is None:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(raw)\n",
    "            valid_json += 1\n",
    "\n",
    "            if int(parsed[\"predicted_stars\"]) == int(row[\"stars\"]):\n",
    "                correct += 1\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": round(correct / total, 3),\n",
    "        \"json_validity\": round(valid_json / total, 3)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02e231a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [05:36<00:00,  1.68s/it]\n",
      "100%|██████████| 200/200 [04:36<00:00,  1.38s/it]\n",
      "100%|██████████| 200/200 [07:37<00:00,  2.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Prompt V1': {'accuracy': 0.695, 'json_validity': 1.0},\n",
       " 'Prompt V2': {'accuracy': 0.65, 'json_validity': 1.0},\n",
       " 'Prompt V3': {'accuracy': 0.625, 'json_validity': 1.0}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {\n",
    "    \"Prompt V1\": evaluate(predict_v1, df),\n",
    "    \"Prompt V2\": evaluate(predict_v2, df),\n",
    "    \"Prompt V3\": evaluate(predict_v3, df)\n",
    "}\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce58b82",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "All three prompting strategies achieved 100% JSON validity after enforcing strict output constraints and post-processing.\n",
    "Prompt V1 achieved the highest accuracy, likely due to its flexibility in interpreting sentiment.\n",
    "Prompt V2 introduced structured reasoning but showed slightly lower accuracy.\n",
    "Prompt V3 prioritized reliability and formatting consistency, resulting in marginally lower accuracy but maximum robustness.\n",
    "\n",
    "This demonstrates the trade-off between strict control and predictive performance when using LLMs for classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1b811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
